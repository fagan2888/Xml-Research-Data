<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>ITR/IM: Capturing, Coordinating and Remembering Human Experience</AwardTitle>
    <AwardEffectiveDate>10/01/2001</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2005</AwardExpirationDate>
    <AwardAmount>2700000</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Stephen Griffin</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This work will develop algorithms and systems enabling people&lt;br/&gt;to query and communicate a synthesized record of&lt;br/&gt;human experiences derived from individual perspectives captured&lt;br/&gt;during selected personal and group activities. For this research,&lt;br/&gt;an experience is defined through what you see, what you hear,&lt;br/&gt;where you are, and associated sensor data and electronic communications.&lt;br/&gt;The research will transform this record into a meaningful, accessible&lt;br/&gt;information resource, available contemporaneously and retrospectively.&lt;br/&gt;We will validate our vision with two societally relevant applications:&lt;br/&gt;(1) providing memory aids as a personal prosthetic or behavioral&lt;br/&gt;monitor for the elderly; and (2) coordinating emergency response&lt;br/&gt;activity in disaster scenarios.&lt;br/&gt;&lt;br/&gt;This project assumes that within ten years technology will be capable&lt;br/&gt;of creating a continuously recorded, digital, high fidelity record of&lt;br/&gt;a person's activities and observations in video form. This research&lt;br/&gt;will prototype personal experience capture units to record audio, video,&lt;br/&gt;location and sensory data, and electronic communications. Each constituent&lt;br/&gt;unit captures, manages, secures and associates information from&lt;br/&gt;its unique point of view. Each operates as a portable, interoperable,&lt;br/&gt;information system, allowing search and retrieval by both its&lt;br/&gt;human operator and remote collaborating systems. An individual&lt;br/&gt;cannot see everything, nor remember everything that was seen or&lt;br/&gt;heard. The integration of multiple points of view provides more&lt;br/&gt;comprehensive coverage of an event, especially when coupled with support&lt;br/&gt;for vastly improving the memory from each perspective. The research thus&lt;br/&gt;enables the following technological advances:&lt;br/&gt;&lt;br/&gt;* Enhanced memory for individuals from an intelligent assistant using an&lt;br/&gt;automatically analyzed and fully indexed archive of captured personal&lt;br/&gt;experiences.&lt;br/&gt;&lt;br/&gt;* Coordination of distributed group activity, such as management of an&lt;br/&gt;emergency response team in a disaster relief situation, utilizing multiple&lt;br/&gt;synchronized streams of incoming observation data to construct a "collective&lt;br/&gt;experience."&lt;br/&gt;&lt;br/&gt;* Expertise synthesized across individuals and maintained over generations,&lt;br/&gt;retrieved and summarized on demand to enable example-based training and&lt;br/&gt;retrospective analysis.&lt;br/&gt;&lt;br/&gt;* Understanding of privacy, security and other societal implications of&lt;br/&gt;ubiquitous experience collection.&lt;br/&gt;&lt;br/&gt;The foundation for this work, the Informedia Digital Video Library,&lt;br/&gt;has demonstrated the successful application of speech, image, and&lt;br/&gt;natural language processing in automatically creating a rich, indexed,&lt;br/&gt;searchable multimedia information resource for broadcast-quality video.&lt;br/&gt;The proposed work builds from these technologies, moving well beyond&lt;br/&gt;a digital video library into new information spaces composed of&lt;br/&gt;unedited personal experience video augmented with additional sensory&lt;br/&gt;and position data. Tools will be created to analyze large amounts&lt;br/&gt;of continuously captured digital experience data in order to extract&lt;br/&gt;salient features, describe scenes and characterize events. The&lt;br/&gt;research will address summarization and collaboration of multiple&lt;br/&gt;simultaneous experiences integrated across time, space and people.</AbstractNarration>
    <MinAmdLetterDate>09/26/2001</MinAmdLetterDate>
    <MaxAmdLetterDate>07/27/2004</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0121641</AwardID>
    <Investigator>
      <FirstName>Howard</FirstName>
      <LastName>Wactlar</LastName>
      <EmailAddress>wactlar@cmu.edu</EmailAddress>
      <StartDate>09/26/2001</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Takeo</FirstName>
      <LastName>Kanade</LastName>
      <EmailAddress>kanade@cs.cmu.edu</EmailAddress>
      <StartDate>09/26/2001</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Alexander</FirstName>
      <LastName>Hauptmann</LastName>
      <EmailAddress>alex@cs.cmu.edu</EmailAddress>
      <StartDate>09/26/2001</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Michael</FirstName>
      <LastName>Christel</LastName>
      <EmailAddress>christel@cmu.edu</EmailAddress>
      <StartDate>09/26/2001</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Mark</FirstName>
      <LastName>Derthick</LastName>
      <EmailAddress>mad@cs.cmu.edu</EmailAddress>
      <StartDate>05/01/2003</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Carnegie-Mellon University</Name>
      <CityName>PITTSBURGH</CityName>
      <ZipCode>152133815</ZipCode>
      <PhoneNumber>4122689527</PhoneNumber>
      <StreetAddress>5000 Forbes Avenue</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Pennsylvania</StateName>
      <StateCode>PA</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0104000</Code>
      <Name>Information Systems</Name>
    </FoaInformation>
    <FoaInformation>
      <Code>0000099</Code>
      <Name>Other Applications NEC</Name>
    </FoaInformation>
    <ProgramElement>
      <Code>1687</Code>
      <Text>ITR MEDIUM (GROUP) GRANTS</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>V068</Code>
      <Text/>
    </ProgramElement>
    <ProgramElement>
      <Code>W295</Code>
      <Text/>
    </ProgramElement>
    <ProgramReference>
      <Code>1655</Code>
      <Text>INFORMATION MANAGEMENT</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9216</Code>
      <Text>ADVANCED SOFTWARE TECH &amp; ALGOR</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9218</Code>
      <Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>HPCC</Code>
      <Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
    </ProgramReference>
  </Award>
</rootTag>
