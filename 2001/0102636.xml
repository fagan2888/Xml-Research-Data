<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Boosting for Regression and Classification: Some Views from Analogy</AwardTitle>
    <AwardEffectiveDate>08/01/2001</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2004</AwardExpirationDate>
    <AwardAmount>74468</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>03040200</Code>
      <Directorate>
        <LongName>Direct For Mathematical &amp; Physical Scien</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Mathematical Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>grace yang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The principal investigator will study theoretical properties of boosting algorithms. Topics include the assumption of weak hypotheses, the behavior of generalization error in the large time limit and during the process of boosting, a comparison to the optimal Bayes error, the performance in noiseless and noisy situations, overfitting and regularization, and the analogy between regression and classification boosting algorithms. The following goals will be addressed: &lt;br/&gt;&lt;br/&gt;(I). Provide conditions and examples for the assumption of weak hypotheses to be valid, as well as some implications of the assumption on the generalization error.&lt;br/&gt;(II). Further understanding of the overfitting behavior and regularization methods in boosting.&lt;br/&gt;(III). Bring together the important recent developments in the areas of regression (e.g., thresholding) and classification (e.g., boosting), where increasingly different sets of tools have been developed. &lt;br/&gt;&lt;br/&gt;Boosting algorithms are very useful tools for combining simple prediction rules sequentially and adaptively into more powerful prediction rules, and are of mutual interest to the fields of computer science, machine learning and statistics. A popular version of the algorithms, called AdaBoost, is shown to improve the fit on the existing data very quickly when more and more relatively simple "rules of thumb" are incorporated. In addition, the algorithm also improves the prediction of new outcomes very effectively. On the other hand, recent empirical evidence has shown that combining too many simple rules can `overfit' the existing data and deteriorate the performance in predicting new, unseen outcomes, when data are `noisy'. This project studies important theoretical properties of boosting algorithms, based on an analogy between the regression situation (when the outcomes are continuous numbers) and the classification situation (when the outcomes are discrete classes). This will be helpful in understanding how boosting works, in what situations, to what degree, and how to prevent `overfitting' and improve the performance when treating noisy data.</AbstractNarration>
    <MinAmdLetterDate>07/25/2001</MinAmdLetterDate>
    <MaxAmdLetterDate>07/25/2001</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>0102636</AwardID>
    <Investigator>
      <FirstName>Wenxin</FirstName>
      <LastName>Jiang</LastName>
      <EmailAddress>wjiang@northwestern.edu</EmailAddress>
      <StartDate>07/25/2001</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Northwestern University</Name>
      <CityName>Evanston</CityName>
      <ZipCode>602013149</ZipCode>
      <PhoneNumber>8474913003</PhoneNumber>
      <StreetAddress>1801 Maple Ave.</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Illinois</StateName>
      <StateCode>IL</StateCode>
    </Institution>
    <FoaInformation>
      <Code>0000099</Code>
      <Name>Other Applications NEC</Name>
    </FoaInformation>
  </Award>
</rootTag>
